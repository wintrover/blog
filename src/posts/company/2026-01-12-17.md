---
layout: post
title: "Testing in the Age of AI Agents: How I Kept QA from Collapsing"
date: 2026-01-12
category: Company Work
tags: Testing QA Automation TDD Pytest Vitest EP-BVA Pairwise State-Transition ISTQB
description: "As AI agents accelerate code output, I adopted practical test design techniques—TDD, EP-BVA, Pairwise, and State Transition—to keep my Orchestrator reliable."
---

AI agents changed my development tempo overnight. I can ship more code in a day than I used to in a week, and that sounds great until the first time a tiny edge case takes down an entire flow.

At that speed, QA becomes either a competitive advantage or a constant fire drill. I chose the first option, and I rebuilt my testing approach in `d:\Coding\Company\Ochestrator` around a small set of test design techniques that scale with code volume:

- TDD
- EP-BVA (Equivalence Partitioning + Boundary Value Analysis)
- Pairwise (Combinatorial Testing)
- State Transition Testing

![Testing Toolbox Diagram](/images/2026-01-12-17-testing-toolbox.png)

---

## 1. Why I Needed “Test Design,” Not Just “More Tests”

When code volume grows, the problem is not only “coverage.” The real problem is that the space of possible inputs and states grows faster than my time.

So I stopped asking:

- “Did I write tests for this function?”

And I started asking:

- “Did I select test cases that actually represent the failure surface?”

That mindset is what pushed me toward structured test design techniques.

---

## 2. TDD: Keeping the Feedback Loop Short

In Orchestrator, a lot of logic is orchestration-heavy: retries, validations, and workflows that fail in non-obvious ways. TDD helped me keep the loop tight:

1. Write a failing test for the scenario I care about.
2. Implement the smallest change to pass.
3. Refactor safely.

This especially mattered for workflow-like code, where a “small change” can shift behavior across multiple steps.

I kept the test execution friction low:

- Backend: `uv run pytest`
- Frontend: `npm run test` (Vitest)

---

## 3. EP-BVA: Turning “Edge Cases” into a Checklist

EP-BVA is my default whenever an input has:

- a clear domain (valid/invalid partitions), and/or
- meaningful boundaries (min/max, thresholds, limits)

One example I actually pinned down in Orchestrator is bcrypt’s 72-byte behavior. I made the boundary explicit and tested just-below, on-the-edge, and just-above:

```python
# apps/backend/tests/test_auth_service.py
def test_password_length_boundaries(self, auth_service):
    p71 = "a" * 71
    h71 = auth_service.get_password_hash(p71)
    assert auth_service.verify_password(p71, h71) is True

    p72 = "a" * 72
    h72 = auth_service.get_password_hash(p72)
    assert auth_service.verify_password(p72, h72) is True

    p73 = "a" * 73
    assert auth_service.verify_password(p73, h72) is True
```

This is exactly the kind of “small but catastrophic” detail that gets missed when I only test the happy path.

---

## 4. Pairwise: When Combinations Explode, I Refuse to Brute Force

Configuration matrices are where QA goes to die:

- providers (CUDA/CPU/OpenVINO)
- model variants
- thresholds
- feature flags

Testing every combination is expensive and usually unnecessary. Pairwise gives me a principled compromise: cover every pair of parameter values at least once, with far fewer test cases.

In Orchestrator I used `allpairspy` for this:

```python
# apps/backend/tests/test_pairwise_configs.py
from allpairspy import AllPairs

parameters = [
    ["CUDA", "CPU", "OpenVINO"],
    ["buffalo_l", "buffalo_m"],
    [0.6, 0.8, 0.9],
    [True, False],
]

pairwise_combinations = list(AllPairs(parameters))
assert len(pairwise_combinations) < 36
```

Pairwise is the technique that lets me keep adding options to the product without turning the test suite into a tax.

---

## 5. State Transition Testing: I Test the Workflow, Not Just Functions

Orchestrator is a system where “state” is the product:

- files move from UPLOADING → PROCESSING → ACTIVE
- failures must land in FAILED
- invalid transitions must be blocked

State Transition Testing gave me a structured way to validate workflows end-to-end, including invalid transitions:

```python
# apps/backend/tests/test_integration_kyc_workflow.py
with pytest.raises(ValueError, match="Invalid status transition"):
    self._validate_transition(ImageStatus.FAILED, ImageStatus.ACTIVE)
```

This matters because bugs in orchestration aren’t usually wrong computations—they are wrong sequences.

---

## Conclusion

In the AI-agent era, code is cheap. Trust is not.

What kept my QA from collapsing was not writing more tests, but adopting test design techniques that scale:

- TDD for fast feedback and safer refactors
- EP-BVA to systematize edge cases
- Pairwise to tame combinatorial growth
- State Transition Testing to validate real workflows

This is the testing toolbox I expect to keep using as my code volume keeps accelerating.
